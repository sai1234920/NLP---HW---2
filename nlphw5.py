# -*- coding: utf-8 -*-
"""NLPHW5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ANx9x7h3pe7wLXTwWFfpwWdfSiObpZ3
"""

# Question 5
import numpy as np

def compute_metrics(conf_mat, labels):
    """
    conf_mat: 2D numpy array where rows = predicted, cols = gold/true
    labels: list of class names in the same order as conf_mat
    """
    conf_mat = np.array(conf_mat, dtype=float)

    # Row sums = predicted counts per class
    pred_counts = conf_mat.sum(axis=1)
    # Column sums = actual counts per class
    true_counts = conf_mat.sum(axis=0)
    # True positives = diagonal
    tp = np.diag(conf_mat)

    # Per-class precision and recall
    precision = np.divide(tp, pred_counts, out=np.zeros_like(tp), where=pred_counts != 0)
    recall = np.divide(tp, true_counts, out=np.zeros_like(tp), where=true_counts != 0)

    # Macro averages
    macro_precision = precision.mean()
    macro_recall = recall.mean()

    # Micro averages
    micro_tp = tp.sum()
    total = conf_mat.sum()
    micro_precision = micro_tp / total if total != 0 else 0.0
    micro_recall = micro_tp / total if total != 0 else 0.0  # same for single-label multi-class

    # Print results
    print("Per-class metrics:")
    for i, label in enumerate(labels):
        print(f"  {label}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}")

    print("\nMacro-averaged:")
    print(f"  Precision = {macro_precision:.4f}")
    print(f"  Recall    = {macro_recall:.4f}")

    print("\nMicro-averaged:")
    print(f"  Precision = {micro_precision:.4f}")
    print(f"  Recall    = {micro_recall:.4f}")


if __name__ == "__main__":
    labels = ["Cat", "Dog", "Rabbit"]

    # rows = predicted (system), cols = gold (true)
    conf_mat = [
        [5, 10, 5],   # predicted Cat
        [15, 20, 10], # predicted Dog
        [0, 15, 10],  # predicted Rabbit
    ]

    compute_metrics(conf_mat, labels)

#Programing question
from collections import Counter
from typing import Dict, List, Tuple

def tokenize(sentence: str) -> List[str]:
    """Tokenize by whitespace; sentences already contain <s> and </s> tokens."""
    return sentence.strip().split()

def build_counts(corpus: List[str]) -> Tuple[Counter, Counter]:
    """
    Build unigram and bigram counts from a corpus.
    Unigram counts are used as C(w_{i-1}) denominators in MLE bigram probabilities.
    """
    unigram = Counter()
    bigram = Counter()

    for sent in corpus:
        toks = tokenize(sent)
        unigram.update(toks)
        for prev, nxt in zip(toks[:-1], toks[1:]):
            bigram[(prev, nxt)] += 1

    return unigram, bigram

def mle_bigram_prob(prev: str, nxt: str, unigram: Counter, bigram: Counter) -> float:
    """MLE: P(nxt | prev) = C(prev,nxt) / C(prev). If unseen prev or bigram => 0."""
    denom = unigram[prev]
    if denom == 0:
        return 0.0
    return bigram[(prev, nxt)] / denom

def sentence_probability(sentence: str, unigram: Counter, bigram: Counter, verbose: bool = True) -> float:
    """
    Compute P(sentence) under the bigram model as product over bigram probabilities.
    If any bigram is unseen, probability becomes 0.0 (no smoothing here).
    """
    toks = tokenize(sentence)
    prob = 1.0

    if verbose:
        print(f"\nSentence: {sentence}")
        print("Bigram probabilities:")

    for prev, nxt in zip(toks[:-1], toks[1:]):
        p = mle_bigram_prob(prev, nxt, unigram, bigram)
        if verbose:
            c_bigram = bigram[(prev, nxt)]
            c_prev = unigram[prev]
            print(f"  P({nxt} | {prev}) = C({prev},{nxt})/C({prev}) = {c_bigram}/{c_prev} = {p:.6f}")
        prob *= p

        # Early stop if probability becomes zero
        if prob == 0.0:
            if verbose:
                print("  -> Unseen bigram encountered, sentence probability becomes 0.")
            break

    if verbose:
        print(f"Total sentence probability = {prob:.12f}")
    return prob

def main():
    # Training corpus (as given)
    corpus = [
        "<s> I love NLP </s>",
        "<s> I love deep learning </s>",
        "<s> deep learning is fun </s>",
    ]

    # Test sentences (as given)
    s1 = "<s> I love NLP </s>"
    s2 = "<s> I love deep learning </s>"

    unigram, bigram = build_counts(corpus)

    # Print counts
    print("Unigram counts:")
    for w, c in unigram.items():
        print(f"  {w}: {c}")

    print("\nBigram counts:")
    for (w1, w2), c in bigram.items():
        print(f"  ({w1}, {w2}): {c}")

    # Compute sentence probabilities
    p1 = sentence_probability(s1, unigram, bigram, verbose=True)
    p2 = sentence_probability(s2, unigram, bigram, verbose=True)

    # Decide which is preferred
    print("\nModel preference:")
    if p1 > p2:
        print(f"  Preferred: S1 because P(S1) = {p1:.12f} > P(S2) = {p2:.12f}")
        print("  Why: S1 has fewer transitions and/or higher-probability bigrams under MLE.")
    elif p2 > p1:
        print(f"  Preferred: S2 because P(S2) = {p2:.12f} > P(S1) = {p1:.12f}")
        print("  Why: S2â€™s bigram transitions are more probable under MLE.")
    else:
        print(f"  Tie: P(S1) = P(S2) = {p1:.12f}")

if __name__ == "__main__":
    main()